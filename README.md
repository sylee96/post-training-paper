# post-training-paper

[A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861)
- Prompting Techniques: 간단한 프롬프트 기법이 유용성, 정직성, 무해성을 향상시킬 수 있으며, 이는 모델 성능 저하 없이 가능.
- Preference Modeling: Imitation learning보다 preference modeling이 우수하며, ranked data에서 특히 효과적.
- Pretraining Preference Models: Stack Exchange, Reddit, Wikipedia 같은 대규모 공개 데이터를 활용한 사전 훈련이 효율성을 크게 높임.
- RLHF PPO 알고리즘을 더 깊게 이해하기 위한 imitation learning에 대해 알아볼 수 있는 논문.
