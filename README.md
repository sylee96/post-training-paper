# post-training-paper

[A General Language Assistant as a Laboratory for Alignment [2112, Anthropic]](https://arxiv.org/pdf/2112.00861)
- Prompting Techniques: 간단한 프롬프트 기법이 유용성, 정직성, 무해성을 향상시킬 수 있으며, 이는 모델 성능 저하 없이 가능.
- Preference Modeling: Imitation learning보다 preference modeling이 우수하며, ranked data에서 특히 효과적.
- Pretraining Preference Models: Stack Exchange, Reddit, Wikipedia 같은 대규모 공개 데이터를 활용한 사전 훈련이 효율성을 크게 높임.
- RLHF PPO 알고리즘을 더 깊게 이해하기 위한 imitation learning에 대해 알아볼 수 있는 논문.

[SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training [2501]](https://arxiv.org/pdf/2501.17161)
- SFT는 암기와 같은 것이라 OOD(Out-of-Distribution)에 대해서는 성능 저하가 올 수 있음.
- RL은 일반화를 하게 해줌. 그래서 visual recognition 능력도 더 좋음.
- 그렇다고 SFT가 필요없는 것은 아님. RL을 안정적으로 학습할 수 있게 포맷을 안정적이게 해줌.


[A Comparative Study on Reasoning Patterns of OpenAI's o1 Model [2410]](https://arxiv.org/pdf/2410.13639)

[s1: Simple test-time scaling [2501]](https://arxiv.org/abs/2501.19393)

[Let's Verify Step by Step [2305, OpenAI]](https://arxiv.org/pdf/2305.20050)

[V-STaR: Training Verifiers for Self-Taught Reasoners [2408, Microsoft]](https://arxiv.org/pdf/2402.06457)


